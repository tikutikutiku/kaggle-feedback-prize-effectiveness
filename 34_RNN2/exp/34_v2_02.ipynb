{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '34_v2_02'\n",
    "MODEL = 'microsoft/deberta-large'\n",
    "LR = 8e-6 \n",
    "HEAD_LR = 8e-6 \n",
    "SEED = 100\n",
    "TRN_BS = 1\n",
    "VAL_BS = 1\n",
    "ACCUM_STEP = 1\n",
    "HIDDEN_DROP_PROB = 0\n",
    "P_DROP = 0\n",
    "RNN = 'lstm'\n",
    "WARMUP_RATIO = 0.1\n",
    "HEAD = 'simple'\n",
    "AUG = 'false' \n",
    "MIXUP_ALPHA = 1.0\n",
    "P_AUG = 0\n",
    "AUG_STOP_EPOCH = 2\n",
    "MSD = 'true'\n",
    "MULTI_LAYERS = 1\n",
    "EVAL_STEP = -1\n",
    "NUM_LABELS = 3\n",
    "NUM_LABELS_2 = 7\n",
    "ADV_SIFT = 'false'\n",
    "FP16 = 'true'\n",
    "WD = 0.01\n",
    "FREEZE = 'false'\n",
    "MULTI_TASK = 'false' \n",
    "W_MT = 1.0 \n",
    "AWP = 'true'\n",
    "AWP_LR = 1e-2\n",
    "AWP_EPS = 1e-3 \n",
    "AWP_START_EPOCH = 1\n",
    "PRETRAINED_DETECTOR_PATH = f'../../input/tascj/result/deberta_large_fold0.pth'\n",
    "MASK_PROB = 0.8\n",
    "MASK_RATIO = 0.3\n",
    "SCHEDULER = 'cosine_hard'\n",
    "CP = 'false'\n",
    "WINDOW_SIZE = 512\n",
    "INNER_LEN = 384\n",
    "EDGE_LEN = 64\n",
    "GRAD_CLIP = 1\n",
    "LOSS_WEIGHT = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-window",
   "metadata": {},
   "source": [
    "# Train with 2021 Pseudo-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "INPUT_PATH = f'../{VERSION}/result/ensemble_pseudo_label_fold{FOLD}.csv'\n",
    "FOLD_PATH = '../../00_EDA/00_v1_13/result/' # for pseudo-label\n",
    "MODE = 'pseudo'\n",
    "LOSS = 'bce'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = 'none'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 1\n",
    "INPUT_PATH = f'../{VERSION}/result/ensemble_pseudo_label_fold{FOLD}.csv'\n",
    "FOLD_PATH = '../../00_EDA/00_v1_13/result/' # for pseudo-label\n",
    "MODE = 'pseudo'\n",
    "LOSS = 'bce' \n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = 'none'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 2\n",
    "INPUT_PATH = f'../{VERSION}/result/ensemble_pseudo_label_fold{FOLD}.csv'\n",
    "FOLD_PATH = '../../00_EDA/00_v1_13/result/' # for pseudo-label\n",
    "MODE = 'pseudo'\n",
    "LOSS = 'bce'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = 'none'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 3\n",
    "INPUT_PATH = f'../{VERSION}/result/ensemble_pseudo_label_fold{FOLD}.csv'\n",
    "FOLD_PATH = '../../00_EDA/00_v1_13/result/' # for pseudo-label\n",
    "MODE = 'pseudo'\n",
    "LOSS = 'bce' \n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = 'none'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 4\n",
    "INPUT_PATH = f'../{VERSION}/result/ensemble_pseudo_label_fold{FOLD}.csv'\n",
    "FOLD_PATH = '../../00_EDA/00_v1_13/result/' # for pseudo-label\n",
    "MODE = 'pseudo'\n",
    "LOSS = 'bce' \n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = 'none'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-brake",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "MODE = 'train'\n",
    "LOSS = 'xentropy'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{EPOCHS}_pseudo.pth'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 1\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "MODE = 'train'\n",
    "LOSS = 'xentropy'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{EPOCHS}_pseudo.pth'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 2\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "MODE = 'train'\n",
    "LOSS = 'xentropy'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{EPOCHS}_pseudo.pth'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 3\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "MODE = 'train'\n",
    "LOSS = 'xentropy'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{EPOCHS}_pseudo.pth'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 4\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "MODE = 'train'\n",
    "LOSS = 'xentropy'\n",
    "EPOCHS = 3\n",
    "STOP_EPOCH = 3\n",
    "RESTART = 1\n",
    "NUM_CYCLES = EPOCHS\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{EPOCHS}_pseudo.pth'\n",
    "\n",
    "!python ../code/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN \\\n",
    "--gradient_clip_val $GRAD_CLIP \\\n",
    "--input_path $INPUT_PATH --mode $MODE --pretrain_path $PRETRAIN_PATH --loss_weight $LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-dispute",
   "metadata": {},
   "source": [
    "# SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../code/swa.py --work_dir ./result/$VERSION/ --fold 0 --seed $SEED --epochs '1 2'\n",
    "!python ../code/swa.py --work_dir ./result/$VERSION/ --fold 1 --seed $SEED --epochs '1 2'\n",
    "!python ../code/swa.py --work_dir ./result/$VERSION/ --fold 2 --seed $SEED --epochs '1 2'\n",
    "!python ../code/swa.py --work_dir ./result/$VERSION/ --fold 3 --seed $SEED --epochs '1 2'\n",
    "!python ../code/swa.py --work_dir ./result/$VERSION/ --fold 4 --seed $SEED --epochs '1 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 768\n",
    "INNER_LEN = 512\n",
    "EDGE_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "LOSS = 'xentropy'\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../code/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--rnn $RNN --loss $LOSS --mt $MULTI_TASK --num_labels $NUM_LABELS --loss $LOSS --weight_path $WEIGHT_PATH \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 1\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/' \n",
    "LOSS = 'xentropy'\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../code/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--rnn $RNN --loss $LOSS --mt $MULTI_TASK --num_labels $NUM_LABELS --loss $LOSS --weight_path $WEIGHT_PATH \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 2\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/' \n",
    "LOSS = 'xentropy'\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../code/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--rnn $RNN --loss $LOSS --mt $MULTI_TASK --num_labels $NUM_LABELS --loss $LOSS --weight_path $WEIGHT_PATH \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 3\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/' \n",
    "LOSS = 'xentropy'\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../code/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--rnn $RNN --loss $LOSS --mt $MULTI_TASK --num_labels $NUM_LABELS --loss $LOSS --weight_path $WEIGHT_PATH \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 4\n",
    "INPUT_PATH = '../../input/feedback-prize-effectiveness/'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/'\n",
    "LOSS = 'xentropy'\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../code/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--rnn $RNN --loss $LOSS --mt $MULTI_TASK --num_labels $NUM_LABELS --loss $LOSS --weight_path $WEIGHT_PATH \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preds = []\n",
    "for fold in range(5):\n",
    "    tmp_df = pd.read_csv(f'./result/{VERSION}/pred_fold{fold}.csv')\n",
    "    preds.append(tmp_df)\n",
    "    \n",
    "pred_df = pd.concat(preds, axis=0).reset_index(drop=True)\n",
    "train_df = pd.read_csv('../../input/feedback-prize-effectiveness/train.csv')\n",
    "\n",
    "oof_df = train_df.merge(pred_df, on='discourse_id', how='left')\n",
    "oof_df.to_csv(f'./result/{VERSION}/oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df.shape, oof_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-collapse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
