{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '18_v2_16'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_07/result/' #'../../00_EDA/00_v1_01/result/'\n",
    "MODEL = 'microsoft/deberta-large'\n",
    "#'microsoft/deberta-xlarge'\n",
    "#'microsoft/deberta-base'\n",
    "#'microsoft/deberta-v3-base'\n",
    "#'funnel-transformer/xlarge'\n",
    "#'ahotrod/electra_large_discriminator_squad2_512'\n",
    "#'funnel-transformer/xlarge'\n",
    "#'microsoft/deberta-v3-large'\n",
    "#'microsoft/deberta-large'\n",
    "#'anferico/bert-for-patents'\n",
    "#'microsoft/deberta-v3-base'\n",
    "#'microsoft/cocolm-large'\n",
    "LR = 8e-6 #2e-5\n",
    "HEAD_LR = 8e-6 #2e-5\n",
    "SEED = 100\n",
    "TRN_BS = 1\n",
    "VAL_BS = 1\n",
    "ACCUM_STEP = 1\n",
    "EPOCHS = 8 #5 #2\n",
    "STOP_EPOCH = 8 #5 #2\n",
    "RESTART = 1\n",
    "HIDDEN_DROP_PROB = 0\n",
    "P_DROP = 0\n",
    "RNN = 'none'\n",
    "WARMUP_RATIO = 0.1\n",
    "LOSS = 'xentropy'\n",
    "HEAD = 'simple'\n",
    "AUG = 'false' #'mixup'\n",
    "MIXUP_ALPHA = 1.0\n",
    "P_AUG = 0\n",
    "AUG_STOP_EPOCH = 2\n",
    "MSD = 'true'\n",
    "MULTI_LAYERS = 1\n",
    "EVAL_STEP = -1 #100\n",
    "NUM_LABELS = 3\n",
    "NUM_LABELS_2 = 7\n",
    "ADV_SIFT = 'false'\n",
    "FP16 = 'true' #'false'\n",
    "WD = 0.01\n",
    "FREEZE = 'false'\n",
    "#MAX_LENGTH = 1024\n",
    "MULTI_TASK = 'false' #'true'\n",
    "W_MT = 1.0 #0.5\n",
    "#PREPROCESSED_DATA_PATH = '../../00_EDA/00_v2_01/result/train.csv'\n",
    "#'../../00_EDA/00_v1_09/result/train.csv'\n",
    "AWP = 'true'\n",
    "AWP_LR = 5e-4 #1e-2 #1.0\n",
    "AWP_EPS = 1e-3 #0.01\n",
    "AWP_START_EPOCH = 1\n",
    "\n",
    "#PRETRAINED_DETECTOR_PATH = f'../../input/tascj/result/deberta_xlarge_fold0.pth'\n",
    "PRETRAINED_DETECTOR_PATH = f'../../input/tascj/result/deberta_large_fold0.pth'\n",
    "#PRETRAINED_DETECTOR_PATH = f'../../05_Detection/exp/result/05_v1_04/model_seed100_fold0_swa.pth'\n",
    "\n",
    "MASK_PROB = 0.8\n",
    "MASK_RATIO = 0.3\n",
    "\n",
    "SCHEDULER = 'cosine_hard'\n",
    "NUM_CYCLES = EPOCHS\n",
    "\n",
    "CP = 'false'\n",
    "\n",
    "WINDOW_SIZE = 1024 #768 #512\n",
    "INNER_LEN = 768 #512 #384\n",
    "EDGE_LEN = 128 #64\n",
    "\n",
    "BNB = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-coating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "train_df.shape =  (36765, 5)\n",
      "test_df.shape =  (10, 4)\n",
      "sub_df.shape =  (10, 4)\n",
      "load folds...\n",
      "trn_df.shape =  (29462, 6)\n",
      "val_df.shape =  (7303, 6)\n",
      "2022-07-23 17:34:37.944344: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-23 17:34:37.944364: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "self.warmup_ratio =  0.1\n",
      "self.num_train_steps =  26816\n",
      "num_warmup_steps =  2681\n",
      "lr :  [0.0, 0.0, 0.0]\n",
      "100%|████████████████████████████| 3352/3352 [22:16<00:00,  2.51it/s, loss=1.53]\n",
      "100%|█████████████████████████████████████████| 839/839 [01:25<00:00,  9.79it/s]\n",
      "epoch 1: trn_loss = 1.5345, val_loss = 0.6627, trn_score = -0.7551, val_score = -0.6627\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "lr :  [7.062615366171966e-06, 7.062615366171966e-06, 7.062615366171966e-06]\n",
      "100%|████████████████████████████| 3352/3352 [22:16<00:00,  2.51it/s, loss=1.26]\n",
      "100%|█████████████████████████████████████████| 839/839 [01:24<00:00,  9.88it/s]\n",
      "epoch 2: trn_loss = 1.2561, val_loss = 0.6568, trn_score = -0.6176, val_score = -0.6568\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "lr :  [5.998196075172701e-06, 5.998196075172701e-06, 5.998196075172701e-06]\n",
      "100%|████████████████████████████| 3352/3352 [22:11<00:00,  2.52it/s, loss=1.16]\n",
      "100%|█████████████████████████████████████████| 839/839 [01:24<00:00,  9.90it/s]\n",
      "epoch 3: trn_loss = 1.1572, val_loss = 0.6214, trn_score = -0.5683, val_score = -0.6214\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "lr :  [4.692883445807095e-06, 4.692883445807095e-06, 4.692883445807095e-06]\n",
      "100%|████████████████████████████| 3352/3352 [22:08<00:00,  2.52it/s, loss=1.06]\n",
      "100%|█████████████████████████████████████████| 839/839 [01:24<00:00,  9.94it/s]\n",
      "epoch 4: trn_loss = 1.0617, val_loss = 0.6069, trn_score = -0.5193, val_score = -0.6069\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "lr :  [3.304039971579417e-06, 3.304039971579417e-06, 3.304039971579417e-06]\n",
      "100%|███████████████████████████| 3352/3352 [22:02<00:00,  2.53it/s, loss=0.965]\n",
      "100%|█████████████████████████████████████████| 839/839 [01:24<00:00,  9.90it/s]\n",
      "epoch 5: trn_loss = 0.9647, val_loss = 0.6285, trn_score = -0.4701, val_score = -0.6285\n",
      "lr :  [1.9990982408787306e-06, 1.9990982408787306e-06, 1.9990982408787306e-06]\n",
      "  0%|                             | 13/3352 [00:05<24:15,  2.29it/s, loss=0.868]^C\n",
      "  0%|                             | 13/3352 [00:06<25:45,  2.16it/s, loss=0.868]\n",
      "Traceback (most recent call last):\n",
      "  File \"../18_v2_16/train.py\", line 148, in <module>\n",
      "    run(args, trn_df, val_df, pseudo_df=None)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/18_BaselineSW/18_v2_16/run.py\", line 190, in run\n",
      "    loss = awp.attack_backward(data)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/18_BaselineSW/18_v2_16/awp.py\", line 35, in attack_backward\n",
      "    _, _, adv_loss = self.model.training_step(inputs)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/18_BaselineSW/18_v2_16/models.py\", line 407, in training_step\n",
      "    logits = self.forward_logits(**input_data)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/18_BaselineSW/18_v2_16/models.py\", line 295, in forward_logits\n",
      "    attention_mask=attention_mask).last_hidden_state\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 959, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 453, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 358, in forward\n",
      "    rel_embeddings=rel_embeddings,\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 297, in forward\n",
      "    attention_output = self.output(self_output, query_states)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 265, in forward\n",
      "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\", line 249, in forward\n",
      "    hidden_states = (hidden_states - mean) / torch.sqrt(variance + self.variance_epsilon)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "FOLD = 0\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN --adam_bits $BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 1\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN --adam_bits $BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 2\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN --adam_bits $BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 3\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN --adam_bits $BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 4\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--epochs $EPOCHS --hidden_drop_prob $HIDDEN_DROP_PROB --p_drop $P_DROP \\\n",
    "--accumulate_grad_batches $ACCUM_STEP --rnn $RNN --warmup_ratio $WARMUP_RATIO --loss $LOSS --aug $AUG --head $HEAD \\\n",
    "--mixup_alpha $MIXUP_ALPHA --p_aug $P_AUG --aug_stop_epoch $AUG_STOP_EPOCH \\\n",
    "--msd $MSD --multi_layers $MULTI_LAYERS --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--num_labels $NUM_LABELS --num_labels_2 $NUM_LABELS_2 \\\n",
    "--restart_epoch $RESTART --adv_sift $ADV_SIFT --fp16 $FP16 --weight_decay $WD --freeze_layers $FREEZE \\\n",
    "--mt $MULTI_TASK --w_mt $W_MT \\\n",
    "--awp $AWP --awp_lr $AWP_LR --awp_eps $AWP_EPS --awp_start_epoch $AWP_START_EPOCH \\\n",
    "--pretrained_detector_path $PRETRAINED_DETECTOR_PATH --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--scheduler $SCHEDULER --num_cycles $NUM_CYCLES --check_pointing $CP \\\n",
    "--window_size $WINDOW_SIZE --inner_len $INNER_LEN --edge_len $EDGE_LEN --adam_bits $BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-sessions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-opposition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
