{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heated-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '05_v2_11'\n",
    "FOLD_PATH = '../../00_EDA/00_v2_05/result/'\n",
    "MODEL = 'microsoft/deberta-v2-xlarge'\n",
    "#'microsoft/deberta-v3-large'\n",
    "#'microsoft/deberta-v3-base'\n",
    "#'funnel-transformer/xlarge'\n",
    "#'ahotrod/electra_large_discriminator_squad2_512'\n",
    "#'funnel-transformer/xlarge'\n",
    "#'microsoft/deberta-large'\n",
    "#'anferico/bert-for-patents'\n",
    "#'microsoft/deberta-v3-base'\n",
    "#'microsoft/cocolm-large'\n",
    "LR = 6e-6 #8e-6 #2e-5\n",
    "HEAD_LR = 6e-6 #8e-6 #2e-4\n",
    "SEED = 100\n",
    "TRN_BS = 1\n",
    "VAL_BS = 1\n",
    "ACCUM_STEP = 1 #4 #16\n",
    "EPOCHS = 1 #6 #2\n",
    "STOP_EPOCH = 1 #6 #2\n",
    "RESTART = 1\n",
    "HIDDEN_DROP_PROB = 0.1\n",
    "P_DROP = 0.5\n",
    "#RNN = 'none'\n",
    "WARMUP_RATIO = 0.05 #0.1\n",
    "#LOSS = 'xentropy'\n",
    "#HEAD = 'simple'\n",
    "#AUG = 'false' #'mixup'\n",
    "#MIXUP_ALPHA = 1.0\n",
    "#P_AUG = 0\n",
    "#AUG_STOP_EPOCH = 2\n",
    "#MSD = 'false'\n",
    "#MULTI_LAYERS = 1\n",
    "EVAL_STEP = -1 #100\n",
    "#NUM_LABELS = 3\n",
    "#NUM_LABELS_2 = 7\n",
    "#ADV_SIFT = 'false'\n",
    "FP16 ='false'\n",
    "WD = 0.01\n",
    "#FREEZE = 'false'\n",
    "#MAX_LENGTH = 1024\n",
    "#MULTI_TASK = 'false'\n",
    "#W_MT = 1.0 #0.5\n",
    "#PREPROCESSED_DATA_PATH = '../../00_EDA/00_v2_01/result/train.csv'\n",
    "#'../../00_EDA/00_v1_09/result/train.csv'\n",
    "#AWP = 'false'\n",
    "#AWP_LR = 1.0\n",
    "#AWP_EPS = 0.01\n",
    "#AWP_START_EPOCH = 1\n",
    "MASK_PROB = 0 #0.8\n",
    "MASK_RATIO = 0 #0.3\n",
    "CP = 'true'\n",
    "SCHEDULER = 'cosine_hard'\n",
    "NUM_CYCLES = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adult-alabama",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "2022-07-25 05:35:38.814129: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-25 05:35:38.814150: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Loaded 12475 samples.\n",
      "Loaded 3119 samples.\n",
      "num_train_steps =  12475\n",
      "self.warmup_ratio =  0.05\n",
      "self.num_train_steps =  12475\n",
      "num_warmup_steps =  623\n",
      "\n",
      "lr :  [0.0, 0.0, 0.0]\n",
      "100%|███████████| 12475/12475 [2:02:31<00:00,  1.70it/s, loss=2.83, score=0.636]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:28<00:00,  3.86it/s]\n",
      "epoch 1: trn_loss = 2.8332, val_loss = 2.3607, trn_score = 0.6362, val_score = 0.7012\n",
      "trn_obj_loss=1.4393, trn_reg_loss=0.4251, trn_cls_loss=0.9688\n",
      "val_obj_loss=1.1980, val_reg_loss=0.3377, val_cls_loss=0.8250\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "epoch_best 1, val_loss_best 2.36065, val_score_best 0.70120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FOLD = 0\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--warmup_ratio $WARMUP_RATIO \\\n",
    "--epochs $EPOCHS --accumulate_grad_batches $ACCUM_STEP --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--restart_epoch $RESTART --fp16 $FP16 --weight_decay $WD --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--check_pointing $CP --scheduler $SCHEDULER --num_cycles $NUM_CYCLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-ebony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continent-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "2022-07-25 12:37:06.325350: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-25 12:37:06.325373: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Loaded 12475 samples.\n",
      "Loaded 3119 samples.\n",
      "num_train_steps =  74850\n",
      "self.warmup_ratio =  0.05\n",
      "self.num_train_steps =  74850\n",
      "num_warmup_steps =  3742\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [0.0, 0.0, 0.0]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:36<00:00, 338.78it/s]\n",
      "\n",
      "lr :  [5.779460325377634e-06, 5.779460325377634e-06, 5.779460325377634e-06]\n",
      "100%|███████████| 12475/12475 [2:03:28<00:00,  1.68it/s, loss=2.42, score=0.713]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:32<00:00,  3.84it/s]\n",
      "epoch 2: trn_loss = 2.4234, val_loss = 2.4269, trn_score = 0.7125, val_score = 0.6962\n",
      "trn_obj_loss=1.2202, trn_reg_loss=0.3542, trn_cls_loss=0.8491\n",
      "val_obj_loss=1.2238, val_reg_loss=0.3521, val_cls_loss=0.8510\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "\n",
      "lr :  [4.776668284971584e-06, 4.776668284971584e-06, 4.776668284971584e-06]\n",
      "100%|███████████| 12475/12475 [2:03:12<00:00,  1.69it/s, loss=2.11, score=0.765]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:34<00:00,  3.83it/s]\n",
      "epoch 3: trn_loss = 2.1136, val_loss = 2.3417, trn_score = 0.7649, val_score = 0.7069\n",
      "trn_obj_loss=1.0661, trn_reg_loss=0.2983, trn_cls_loss=0.7492\n",
      "val_obj_loss=1.1645, val_reg_loss=0.3373, val_cls_loss=0.8398\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "\n",
      "lr :  [3.247703276126165e-06, 3.247703276126165e-06, 3.247703276126165e-06]\n",
      "  5%|▋            | 606/12475 [05:59<1:34:05,  2.10it/s, loss=1.87, score=0.802]^C\n",
      "  5%|▋            | 606/12475 [05:59<1:57:29,  1.68it/s, loss=1.87, score=0.802]\n",
      "Traceback (most recent call last):\n",
      "  File \"../05_v2_11/train.py\", line 151, in <module>\n",
      "    run(args, trn_df, val_df, pseudo_df=None)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/run.py\", line 175, in run\n",
      "    trn_res = model.training_step(data, trn_df, test_score_thr=args.test_score_thr)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/models.py\", line 240, in training_step\n",
      "    res = self.predict(data, test_score_thr)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/models.py\", line 205, in predict\n",
      "    if pos_inds.sum() == 0:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "FOLD = 0\n",
    "\n",
    "EPOCHS = 6 #2\n",
    "STOP_EPOCH = 6 #2\n",
    "RESTART = 2\n",
    "\n",
    "MASK_PROB = 0.5 #0.8\n",
    "MASK_RATIO = 0.15 #0.3\n",
    "\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{RESTART-1}.pth'\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--warmup_ratio $WARMUP_RATIO \\\n",
    "--epochs $EPOCHS --accumulate_grad_batches $ACCUM_STEP --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--restart_epoch $RESTART --fp16 $FP16 --weight_decay $WD --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--check_pointing $CP --scheduler $SCHEDULER --num_cycles $NUM_CYCLES \\\n",
    "--pretrain_path $PRETRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intense-metabolism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "2022-07-25 17:46:23.652699: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-25 17:46:23.652721: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Loaded 12475 samples.\n",
      "Loaded 3119 samples.\n",
      "num_train_steps =  74850\n",
      "self.warmup_ratio =  0.05\n",
      "self.num_train_steps =  74850\n",
      "num_warmup_steps =  3742\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [0.0, 0.0, 0.0]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:36<00:00, 344.45it/s]\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [5.779460325377634e-06, 5.779460325377634e-06, 5.779460325377634e-06]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:37<00:00, 336.55it/s]\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [4.776668284971584e-06, 4.776668284971584e-06, 4.776668284971584e-06]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:37<00:00, 335.87it/s]\n",
      "\n",
      "lr :  [3.247703276126165e-06, 3.247703276126165e-06, 3.247703276126165e-06]\n",
      "100%|███████████| 12475/12475 [1:58:27<00:00,  1.76it/s, loss=3.99, score=0.417]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:29<00:00,  3.85it/s]\n",
      "epoch 4: trn_loss = 3.9909, val_loss = 2.3419, trn_score = 0.4167, val_score = 0.7100\n",
      "trn_obj_loss=2.4258, trn_reg_loss=0.4731, trn_cls_loss=1.0920\n",
      "val_obj_loss=1.2460, val_reg_loss=0.3007, val_cls_loss=0.7952\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "\n",
      "lr :  [1.6453791775767563e-06, 1.6453791775767563e-06, 1.6453791775767563e-06]\n",
      "100%|███████████| 12475/12475 [2:00:23<00:00,  1.73it/s, loss=3.67, score=0.447]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:25<00:00,  3.87it/s]\n",
      "epoch 5: trn_loss = 3.6730, val_loss = 2.4742, trn_score = 0.4471, val_score = 0.7105\n",
      "trn_obj_loss=2.2714, trn_reg_loss=0.4512, trn_cls_loss=0.9505\n",
      "val_obj_loss=1.3298, val_reg_loss=0.3109, val_cls_loss=0.8336\n",
      "model (best score) saved\n",
      "\n",
      "lr :  [4.4423568503767087e-07, 4.4423568503767087e-07, 4.4423568503767087e-07]\n",
      "  0%|               | 36/12475 [00:19<2:13:42,  1.55it/s, loss=4.3, score=0.306]^C\n",
      "  0%|               | 36/12475 [00:19<1:54:22,  1.81it/s, loss=4.3, score=0.306]\n",
      "Traceback (most recent call last):\n",
      "  File \"../05_v2_11/train.py\", line 151, in <module>\n",
      "    run(args, trn_df, val_df, pseudo_df=None)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/run.py\", line 175, in run\n",
      "    trn_res = model.training_step(data, trn_df, test_score_thr=args.test_score_thr)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/models.py\", line 240, in training_step\n",
      "    res = self.predict(data, test_score_thr)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/models.py\", line 195, in predict\n",
      "    obj_pred, reg_pred, cls_pred = self.forward_logits(data)\n",
      "  File \"/home/takesako2/programming/kaggle/54_Feedback2/05_Detection/05_v2_11/models.py\", line 181, in forward_logits\n",
      "    attention_mask=data['attention_mask']).last_hidden_state\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1054, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 465, in forward\n",
      "    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 446, in get_rel_pos\n",
      "    q, hidden_states.size(-2), bucket_size=self.position_buckets, max_position=self.max_relative_positions\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 564, in build_relative_position\n",
      "    rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n",
      "  File \"/home/takesako2/anaconda3/lib/python3.7/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 537, in make_log_bucket_position\n",
      "    log_pos = np.ceil(np.log(abs_pos / mid) / np.log((max_position - 1) / mid) * (mid - 1)) + mid\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# FOLD = 0\n",
    "\n",
    "# EPOCHS = 6 #2\n",
    "# STOP_EPOCH = 6 #2\n",
    "# RESTART = 4\n",
    "\n",
    "# MASK_PROB = 0.5\n",
    "# MASK_RATIO = 0.3\n",
    "\n",
    "# PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{RESTART-1}.pth'\n",
    "\n",
    "# !python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "# --lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "# --warmup_ratio $WARMUP_RATIO \\\n",
    "# --epochs $EPOCHS --accumulate_grad_batches $ACCUM_STEP --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "# --restart_epoch $RESTART --fp16 $FP16 --weight_decay $WD --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "# --check_pointing $CP --scheduler $SCHEDULER --num_cycles $NUM_CYCLES \\\n",
    "# --pretrain_path $PRETRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-posting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coated-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "2022-07-25 22:26:08.530787: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-25 22:26:08.530809: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Loaded 12475 samples.\n",
      "Loaded 3119 samples.\n",
      "num_train_steps =  74850\n",
      "self.warmup_ratio =  0.05\n",
      "self.num_train_steps =  74850\n",
      "num_warmup_steps =  3742\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [0.0, 0.0, 0.0]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:36<00:00, 338.26it/s]\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [5.779460325377634e-06, 5.779460325377634e-06, 5.779460325377634e-06]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:37<00:00, 331.84it/s]\n",
      "  0%|                                                 | 0/12475 [00:00<?, ?it/s]lr :  [4.776668284971584e-06, 4.776668284971584e-06, 4.776668284971584e-06]\n",
      "100%|████████████████████████████████████| 12475/12475 [00:37<00:00, 334.96it/s]\n",
      "\n",
      "lr :  [3.247703276126165e-06, 3.247703276126165e-06, 3.247703276126165e-06]\n",
      "100%|████████████| 12475/12475 [2:02:28<00:00,  1.70it/s, loss=1.94, score=0.79]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:23<00:00,  3.88it/s]\n",
      "epoch 4: trn_loss = 1.9419, val_loss = 2.3355, trn_score = 0.7897, val_score = 0.7136\n",
      "trn_obj_loss=0.9875, trn_reg_loss=0.2683, trn_cls_loss=0.6862\n",
      "val_obj_loss=1.2413, val_reg_loss=0.2987, val_cls_loss=0.7955\n",
      "model (best loss) saved\n",
      "model (best score) saved\n",
      "\n",
      "lr :  [1.6453791775767563e-06, 1.6453791775767563e-06, 1.6453791775767563e-06]\n",
      "100%|███████████| 12475/12475 [2:00:49<00:00,  1.72it/s, loss=1.76, score=0.819]\n",
      "100%|███████████████████████████████████████| 3119/3119 [12:55<00:00,  4.02it/s]\n",
      "epoch 5: trn_loss = 1.7553, val_loss = 2.4223, trn_score = 0.8189, val_score = 0.7098\n",
      "trn_obj_loss=0.8951, trn_reg_loss=0.2368, trn_cls_loss=0.6234\n",
      "val_obj_loss=1.2988, val_reg_loss=0.3047, val_cls_loss=0.8188\n",
      "\n",
      "lr :  [4.4423568503767087e-07, 4.4423568503767087e-07, 4.4423568503767087e-07]\n",
      "100%|███████████| 12475/12475 [2:02:10<00:00,  1.70it/s, loss=1.66, score=0.833]\n",
      "100%|███████████████████████████████████████| 3119/3119 [13:21<00:00,  3.89it/s]\n",
      "epoch 6: trn_loss = 1.6558, val_loss = 2.4699, trn_score = 0.8328, val_score = 0.7109\n",
      "trn_obj_loss=0.8414, trn_reg_loss=0.2234, trn_cls_loss=0.5909\n",
      "val_obj_loss=1.3303, val_reg_loss=0.3062, val_cls_loss=0.8333\n",
      "epoch_best 4, val_loss_best 2.33546, val_score_best 0.71362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FOLD = 0\n",
    "\n",
    "EPOCHS = 6 #2\n",
    "STOP_EPOCH = 6 #2\n",
    "RESTART = 4\n",
    "\n",
    "MASK_PROB = 0.8\n",
    "MASK_RATIO = 0.22\n",
    "\n",
    "PRETRAIN_PATH = f'result/{VERSION}/model_seed{SEED}_fold{FOLD}_epoch{RESTART-1}.pth'\n",
    "\n",
    "!python ../$VERSION/train.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED \\\n",
    "--lr $LR --head_lr $HEAD_LR --trn_batch_size $TRN_BS --val_batch_size $VAL_BS \\\n",
    "--warmup_ratio $WARMUP_RATIO \\\n",
    "--epochs $EPOCHS --accumulate_grad_batches $ACCUM_STEP --eval_step $EVAL_STEP --stop_epoch $STOP_EPOCH \\\n",
    "--restart_epoch $RESTART --fp16 $FP16 --weight_decay $WD --mask_prob $MASK_PROB --mask_ratio $MASK_RATIO \\\n",
    "--check_pointing $CP --scheduler $SCHEDULER --num_cycles $NUM_CYCLES \\\n",
    "--pretrain_path $PRETRAIN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-council",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excited-cylinder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "2022-07-26 05:14:00.663939: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-26 05:14:00.663959: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loaded 3119 samples.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|                                                  | 0/3119 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 3119/3119 [09:29<00:00,  5.48it/s]\n",
      "val_loss=2.3354, val_score=0.7136\n",
      "val_obj_loss=1.2413, val_reg_loss=0.2987, val_cls_loss=0.7955\n",
      "save raw_oof_fold0...\n",
      "save raw_oof_fold0, done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SCORE_THR = 0.5\n",
    "\n",
    "!python ../$VERSION/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold 0 --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--test_score_thr $TEST_SCORE_THR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "catholic-istanbul",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "2022-07-26 05:23:56.829661: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-26 05:23:56.829681: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loaded 3119 samples.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                  | 0/3119 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 3119/3119 [09:25<00:00,  5.52it/s]\n",
      "val_loss=2.3355, val_score=0.7153\n",
      "val_obj_loss=1.2413, val_reg_loss=0.2987, val_cls_loss=0.7955\n",
      "save raw_oof_fold0...\n",
      "save raw_oof_fold0, done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SCORE_THR = 0.6 #0.5\n",
    "\n",
    "!python ../$VERSION/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold 0 --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--test_score_thr $TEST_SCORE_THR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-runner",
   "metadata": {},
   "source": [
    "# SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "powered-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../$VERSION/swa.py --work_dir ./result/$VERSION/ --fold 0 --seed $SEED --start_epoch 2 --end_epoch 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-visibility",
   "metadata": {},
   "source": [
    "# Validation with SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "comparable-german",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.2\n",
      "2022-07-26 06:55:27.768495: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-07-26 06:55:27.768515: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "train_df.shape =  (144293, 8)\n",
      "load folds...\n",
      "trn_df.shape =  (115526, 8)\n",
      "val_df.shape =  (28767, 8)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loaded 3119 samples.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                  | 0/3119 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 3119/3119 [09:17<00:00,  5.59it/s]\n",
      "val_loss=2.3569, val_score=0.7173\n",
      "val_obj_loss=1.2165, val_reg_loss=0.3184, val_cls_loss=0.8220\n",
      "save raw_oof_fold0...\n",
      "save raw_oof_fold0, done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SCORE_THR = 0.6 #0.5\n",
    "\n",
    "FOLD = 0\n",
    "WEIGHT_PATH = f'./result/{VERSION}/model_seed{SEED}_fold{FOLD}_swa.pth'\n",
    "\n",
    "!python ../$VERSION/validation.py --model $MODEL --version $VERSION --fold_path $FOLD_PATH --fold $FOLD --seed $SEED --val_batch_size $VAL_BS \\\n",
    "--test_score_thr $TEST_SCORE_THR --weight_path $WEIGHT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-mobile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-document",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-uruguay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
